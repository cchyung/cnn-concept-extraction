{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as im\n",
    "from PIL import Image\n",
    "import skimage\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import re\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n",
    "from sklearn import linear_model\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import pickle\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'dataset/broden1_227/'\n",
    "df = pd.read_csv(data_dir + 'processed_index.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_category(df, category, train):\n",
    "    pattern = \"[%i]\" % category\n",
    "    split = 'train' if train else 'val'\n",
    "    return df[df['features'].str.contains(pattern, regex=False) & df['split'].str.match(split)].index\n",
    "    \n",
    "def filter_by_not_category(df, category, train):\n",
    "    pattern = \"[%i]\" % category\n",
    "    split = 'train' if train else 'val'\n",
    "    return df[~df['features'].str.contains(pattern, regex=False) & df['split'].str.match(split)].index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model initialized\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet50(pretrained=True)\n",
    "\n",
    "model.to(device)\n",
    "print(\"model initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = []\n",
    "    \n",
    "def layer_hook(module, input, output):\n",
    "    layer_output = output.data.cpu()\n",
    "    m = nn.AvgPool2d(4)\n",
    "    layer_output = m(layer_output)\n",
    "    outputs.append(layer_output)\n",
    "# handle = model.layer1[2].bn3.register_forward_hook(layer_hook)\n",
    "# handle = model.layer2[3].bn3.register_forward_hook(layer_hook)\n",
    "# handle = model.layer3[5].bn3.register_forward_hook(layer_hook)\n",
    "handle = model.layer4[2].bn3.register_forward_hook(layer_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryDataSet(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.dataframe = df\n",
    "        self.root_dir = img_dir\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        imgfile = os.path.join(self.root_dir,\n",
    "                                self.dataframe.at[idx, 'image'])\n",
    "        image = Image.open(imgfile)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        sample = (image, idx)\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = transforms.Compose([transforms.Resize(224), transforms.ToTensor()])\n",
    "\n",
    "dataset = CategoryDataSet(df, data_dir + 'images/', transform=tf)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=25, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_MAX_INDEX = 44403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2533/2533 [04:09<00:00, 10.16it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "# run dataset through model\n",
    "for inputs, idx in tqdm(dataloader):\n",
    "    inputs = inputs.to(device)\n",
    "    model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_outputs(outputs):\n",
    "    stacked_outputs = torch.cat(outputs, dim=0)\n",
    "#     m = nn.AvgPool2d(4)\n",
    "#     max_pool_outputs = m(stacked_outputs)\n",
    "    max_pool_outputs = stacked_outputs\n",
    "    flattened_outputs = max_pool_outputs.reshape((max_pool_outputs.shape[0], np.prod(max_pool_outputs.shape[1:])))\n",
    "    normalized_outputs = normalize(flattened_outputs)\n",
    "    return normalized_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'layer4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"outputs/%s.pickle\" % layer, 'wb') as handle:\n",
    "#     pickle.dump(outputs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_outputs = np.concatenate(outputs)\n",
    "max_pool_outputs = stacked_outputs\n",
    "flattened_outputs = max_pool_outputs.reshape((max_pool_outputs.shape[0], np.prod(max_pool_outputs.shape[1:])))\n",
    "normalized_outputs = normalize(flattened_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63305, 2048)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform all activations\n",
    "# transformed_outputs = transform_outputs(outputs)\n",
    "# transformed_outputs = transform_outputs(outputs['layer4'])\n",
    "pca = PCA(n_components=100)\n",
    "pca.fit(normalized_outputs[:TRAIN_MAX_INDEX]) # just do it on training data\n",
    "\n",
    "with open(\"pca/%s.pickle\" % layer, 'wb') as handle:\n",
    "    pickle.dump(pca, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_outputs = pca.transform(normalized_outputs)\n",
    "outputs = transformed_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63305, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD_MAX_ITERATIONS = 400\n",
    "SGD_LOSS = \"hinge\"\n",
    "SGD_PENALTY = \"l2\"\n",
    "\n",
    "import random\n",
    "\n",
    "def train_clf_for_category(df, category, activations):\n",
    "    # Filter out category data from dataframe\n",
    "    pos_train_indices = filter_by_category(df, category, True)\n",
    "    negative_train_indices = filter_by_not_category(df,category, True)\n",
    "    pos_val_indices = filter_by_category(df, category, False)\n",
    "    negative_val_indices = filter_by_not_category(df,category, False)        \n",
    "    \n",
    "    if(len(pos_train_indices) == 0):\n",
    "        print(\"Empty set found for category %i\" % category)\n",
    "        return(None, 0)\n",
    "    \n",
    "    # generate balacnced train set\n",
    "    pos_train = [activations[i] for i in pos_train_indices]\n",
    "    negative_train = [activations[i] for i in random.sample(negative_train_indices, min(len(pos_train_indices), len(negative_train_indices)))]\n",
    "    pos_val = [activations[i] for i in pos_val_indices]\n",
    "    negative_val = [activations[i] for i in random.sample(negative_val_indices, min(len(pos_val_indices), len(negative_val_indices)))]\n",
    "    \n",
    "    # generate training and validation data for SGDClassifier\n",
    "    train = pos_train + negative_train\n",
    "    val = pos_val + negative_val\n",
    "    \n",
    "    train_labels = np.append(np.ones(len(pos_train)), np.zeros(len(negative_train)))\n",
    "    val_labels = np.append(np.ones(len(pos_val)), np.zeros(len(negative_val)))\n",
    "    clf = linear_model.SGDClassifier(max_iter=min(np.ceil(10**6 / len(train_labels)), SGD_MAX_ITERATIONS), loss=SGD_LOSS, early_stopping=True, penalty=SGD_PENALTY, average=True, eta0=1.5)\n",
    "    clf.fit(train, train_labels)\n",
    "    return (clf, clf.score(val, val_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/conner/anaconda3/envs/torch2/lib/python2.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9791016348341358\n"
     ]
    }
   ],
   "source": [
    "clf, score = train_clf_for_category(df, 1, outputs)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosen based on frequency in the dataset\n",
    "MAX_CLASSES = 150\n",
    "SCORE_THRESH = 0.75\n",
    "sgd_classifiers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ignoring clf for category 12 with score 0.652450\n",
      "ignoring clf for category 14 with score 0.703454\n",
      "ignoring clf for category 15 with score 0.722811\n",
      "ignoring clf for category 17 with score 0.721992\n",
      "ignoring clf for category 20 with score 0.709237\n",
      "ignoring clf for category 22 with score 0.692779\n",
      "ignoring clf for category 23 with score 0.707806\n",
      "ignoring clf for category 24 with score 0.720226\n",
      "ignoring clf for category 27 with score 0.715784\n",
      "ignoring clf for category 28 with score 0.707865\n",
      "ignoring clf for category 30 with score 0.718845\n",
      "ignoring clf for category 34 with score 0.741709\n",
      "ignoring clf for category 35 with score 0.729223\n",
      "ignoring clf for category 36 with score 0.701427\n",
      "ignoring clf for category 41 with score 0.744082\n",
      "ignoring clf for category 45 with score 0.737057\n",
      "ignoring clf for category 46 with score 0.714679\n",
      "ignoring clf for category 48 with score 0.721154\n",
      "ignoring clf for category 49 with score 0.717349\n",
      "Empty set found for category 59\n",
      "ignoring clf for category 59 with score 0.000000\n",
      "ignoring clf for category 63 with score 0.721947\n",
      "ignoring clf for category 64 with score 0.699821\n",
      "ignoring clf for category 65 with score 0.657166\n",
      "ignoring clf for category 69 with score 0.693220\n",
      "ignoring clf for category 70 with score 0.677632\n",
      "ignoring clf for category 71 with score 0.685639\n",
      "ignoring clf for category 72 with score 0.731947\n",
      "ignoring clf for category 76 with score 0.702882\n",
      "ignoring clf for category 78 with score 0.714859\n",
      "ignoring clf for category 79 with score 0.745798\n",
      "ignoring clf for category 80 with score 0.711297\n",
      "ignoring clf for category 81 with score 0.734151\n",
      "Empty set found for category 83\n",
      "ignoring clf for category 83 with score 0.000000\n",
      "ignoring clf for category 84 with score 0.712695\n",
      "ignoring clf for category 86 with score 0.721258\n",
      "ignoring clf for category 89 with score 0.636253\n",
      "ignoring clf for category 90 with score 0.727848\n",
      "ignoring clf for category 100 with score 0.719020\n",
      "ignoring clf for category 107 with score 0.694056\n",
      "ignoring clf for category 109 with score 0.712062\n",
      "ignoring clf for category 111 with score 0.679134\n",
      "ignoring clf for category 112 with score 0.626866\n",
      "ignoring clf for category 113 with score 0.716270\n",
      "ignoring clf for category 119 with score 0.681655\n",
      "ignoring clf for category 121 with score 0.729258\n",
      "ignoring clf for category 122 with score 0.694981\n",
      "ignoring clf for category 124 with score 0.649083\n",
      "Empty set found for category 126\n",
      "ignoring clf for category 126 with score 0.000000\n",
      "ignoring clf for category 127 with score 0.737395\n",
      "ignoring clf for category 129 with score 0.683824\n",
      "Empty set found for category 130\n",
      "ignoring clf for category 130 with score 0.000000\n",
      "ignoring clf for category 132 with score 0.677350\n",
      "Empty set found for category 134\n",
      "ignoring clf for category 134 with score 0.000000\n",
      "ignoring clf for category 136 with score 0.748768\n",
      "ignoring clf for category 138 with score 0.739011\n",
      "ignoring clf for category 144 with score 0.717647\n",
      "ignoring clf for category 145 with score 0.611111\n",
      "ignoring clf for category 147 with score 0.736264\n",
      "ignoring clf for category 148 with score 0.675000\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, MAX_CLASSES):\n",
    "    clf, score = train_clf_for_category(df, i, outputs)\n",
    "    if(score > SCORE_THRESH):\n",
    "        sgd_classifiers[i] = (clf, score)\n",
    "    else:\n",
    "        print(\"ignoring clf for category %i with score %f\" % (i, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8283000747152544\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "multiplied_accuracy = 1\n",
    "for key, value in sgd_classifiers.items():\n",
    "    scores.append(value[1])\n",
    "    multiplied_accuracy = multiplied_accuracy * value[1]\n",
    "\n",
    "print (reduce(lambda x, y: x + y, scores) / len(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(SGDClassifier(alpha=0.0001, average=True, class_weight=None,\n",
       "        early_stopping=True, epsilon=0.1, eta0=1.5, fit_intercept=True,\n",
       "        l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=23.0,\n",
       "        n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "        power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
       "        validation_fraction=0.1, verbose=0, warm_start=False),\n",
       " 0.9791016348341358)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_classifiers.get(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Store data (serialize)\n",
    "with open('classifiers-%s.pickle' % layer, 'wb') as handle:\n",
    "    pickle.dump(sgd_classifiers, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len (sgd_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch2)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
